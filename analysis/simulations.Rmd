---
title: "simulations"
author: "karltayeb"
date: "2021-04-13"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

# Simulations

## Overview

Here we show the results of 3 simple simulations.

*   Single active gene set, gene in gene list iff gene in active gene set
*   Single active gene set: gene set genes are included in gene list according to `active.logit`, background genes included in gene set according to `background.logit`
*   Multiple active gene sets: 5 or 10 active gene sets per simulation. We simulate under an additive model with intercept determined by `background.logit` and with constant effect size parameterized by `active.logit`


We compare to Fisher's exact test, Regularized regression: elastic net, lasso, and sparse Bayesian regression: logistic SuSiE, SuSiE, Mr. Ash (initilized with a lasso prior)

## Summary

*   SuSiE and logistic SuSiE worked better than Mr. Ash, but that is possible an issue of setting the priors right.
*   Logistic SuSiE's PIPs are better calibrated that SuSiE. Although this is not surprising
*   We reproduce the performance of elastic net seen in `gerr` but we note that the advantages are not as large when using smaller, curated gene sets. Also both elastic net and lasso had low precision compared to SuSiE, logistic SuSiE and Mr. ASH


```{r}
library(targets)
library(tidyverse)

# load and row_bind all targets matching a pattern
tar_agg <- function(pattern){
  load.env <- new.env()
  tar_load(matches(pattern), envir = load.env)
  result <- grep(pattern , names(load.env), value=TRUE)
  result <- do.call("list", mget(result, envir = load.env))
  result <- bind_rows(result)
}
```

# Fully observed gene set

Let's look at how different methods perform identifying a single, fully observed gene set. That is $y_i = 1 \iff g_i \in G$ where $G$ is the gene set of interest. Regression methods should get this perfectly, you can perfectly fit the gene list with $y = {\bf e}_j X$. It's interesting that elastic net fails at this.

The question here is pretty simple, do we detect the true active gene set? And how many other gene sets does each method report?

```{r}
pattern <- '^c2.single.geneset.score_([^_])*$'
result <- tar_agg(pattern)

# plot histogram number of false positives
result %>%
  unnest(scores) %>%
  mutate(thresh = ifelse(method %in% c('elastic.net', 'lasso'), thresh + 0.95, thresh)) %>%
  filter(near(thresh, 0.95)) %>%
  group_by(method) %>%
  mutate(FP = ifelse(FP > 5, 5, FP)) %>%
  group_by(method, FP) %>% summarise(freq = length(FP)) %>%
  ggplot(aes(x=factor(FP), y=freq, fill=method)) + geom_bar(stat="identity") + facet_wrap(vars(method))

# plot
result %>%
  unnest(scores) %>%
  mutate(thresh = ifelse(method %in% c('elastic.net', 'lasso'), thresh + 0.95, thresh)) %>%
  filter(near(thresh, 0.95)) %>%
  group_by(method, TP) %>% summarise(freq = length(TP)) %>%
  ggplot(aes(x=factor(TP), y=freq, fill=method)) + geom_bar(stat="identity") + facet_wrap(vars(method))
```

Elastic net and Fisher's exact test are the only ones that return False positives.
`mr.ash.lasso` is the only one that misses a few of the gene sets. This is odd because it's initialized with lasso.

### Compare SuSiE with VEB boost implimentation

I found that SuSiE was running slower than `veb.boost` when using larger gene set matrices. This is just a check to demonstrate that the PIPs agree.

They do not agree perfectly, but for now I will go ahead using `susie.veb.boost`

```{r}
tar_load(cas.fit_susie, branches = 2)
tar_load(cas.fit_susie.veb.boost, branches = 2)

pip.comp <- tibble(
  susie.pip = cas.fit_susie$pip,
  susie.veb.boost.pip = cas.fit_susie.veb.boost$pip
)

pip.comp %>% unnest(everything()) %>% ggplot(aes(x=susie.pip, y=susie.veb.boost.pip)) + geom_point()
```


## Single active gene set

Now we simulate one active gene set. The log odds of observing the genes in the active gene set are controlled by `active.logit` and the log odds of observing background genes is controlled by `background.logit`

We plot FDP vs power. Dot's indicate 0.95 thershold. We threshold PIPs for SuSiE, logistic SuSiE, and Mr. ASH. $1 - p_{BH}$ for FET, where $p_{BH}$ is the Benjamini Hochberg corrected p-value.

```{r}
pattern <- '^l1.sim.score_([^_])*$'
result <- tar_agg(pattern)

plot.data <- result %>%
  unnest(scores) %>%
  group_by(method, thresh) %>%
  arrange(dplyr::desc(thresh)) %>%
  mutate(power = sum(TP) / sum(TP + FN), FDP = sum(FP) / sum(FP + TP))

ggplot(data=plot.data, aes(x=FDP, y=power, color=method)) + 
  geom_path() + 
  geom_point(data=subset(plot.data, near(thresh, 0.95)))
```

Over the range of simulation we see that Mr. ASH has less power compared to SuSiE and logistic SuSiE. SuSiE's power/FDR curve is generally more favorable than logistic SuSiE but the PIPs are not as well calibrated. Logistic SuSiE consistently controls FDP while SuSiE does not. 

Lasso, elastic net, and Fisher's exact test all detect lots of active gene sets, but also a lot of false positives.

A more granual view: Rows are increasing log odds of observing genes in active gene sets. Columns are increasing log odds of observing background genes. Most of these simulation scenarios. Points indicate a PIP threshold of 0.95 (for Fisher's exact test, $< 0.05$ Benjamini Hochberg corrected p-value). Logistic SuSiE and SuSiE do better than Mr. Ash overall. Logistic SuSiE often has lower power at the cutoff, but reliably keeps proportion of false discoveries low.

```{r}
plot.data <- result %>%
  unnest(scores) %>%
  group_by(method, background.logit, active.logit, thresh) %>%
  arrange(dplyr::desc(thresh)) %>%
  mutate(power = sum(TP) / sum(TP + FN), FDP = sum(FP) / sum(FP + TP))

ggplot(data=plot.data, aes(x=FDP, y=power, color=method)) + 
  geom_path() + 
  geom_point(data=subset(plot.data, near(thresh, 0.95))) +
  facet_grid(vars(active.logit), vars(background.logit)) + theme(aspect=1.0)
```


This only is a little easier to look at, each facet is a different setting of `background.logit`. Interestingly, SuSiE FDP is worse when there are few background genes in the gene list.

```{r}
plot.data <- result %>%
  unnest(scores) %>%
  group_by(method, background.logit, thresh) %>%
  arrange(dplyr::desc(thresh)) %>%
  mutate(power = sum(TP) / sum(TP + FN), FDP = sum(FP) / sum(FP + TP))

ggplot(data=plot.data, aes(x=FDP, y=power, color=method)) + 
  geom_path() + 
  geom_point(data=subset(plot.data, near(thresh, 0.95))) +
  facet_wrap(vars(background.logit)) + theme(aspect=1.0)
```


Again but now facet on `active.logit`.

```{r}
plot.data <- result %>%
  filter(!(method %in% c('elastic.net', 'lasso'))) %>%
  unnest(scores) %>%
  group_by(method, active.logit, thresh) %>%
  arrange(dplyr::desc(thresh)) %>%
  mutate(power = sum(TP) / sum(TP + FN), FDP = sum(FP) / sum(FP + TP))

ggplot(data=plot.data, aes(x=FDP, y=power, color=method)) + 
  geom_path() + 
  geom_point(data=subset(plot.data, near(thresh, 0.95))) +
  facet_wrap(vars(active.logit)) + theme(aspect=1.0)
```


Precision, recall, F1, specificity at a 0.95 cutoff. Bottom right corner there is no enrichment.
Mr. Ash is often not reporting any gene sets when `active.logit` is small. We also see that SuSiE has higher recall than logistic SuSiE both when `background.logit` is large and `active.logit` is small.

```{r}
t <- 0.95
result %>%
  unnest(scores) %>%
  filter(near(thresh, t)) %>%
  group_by(method, background.logit, active.logit) %>%
  summarise(
    recall = sum(TP) / sum(TP + FN),
    precision = sum(TP) / sum(TP + FP),
    specificity = sum(TN) / sum(TN + FP),
    F1 = 2 * precision * recall / (precision + recall)
  ) %>%
  pivot_longer(c(recall, precision, specificity, F1), names_to= 'metric') %>%
  ggplot(aes(x=background.logit, y=active.logit, fill=value)) + 
  geom_tile() + facet_grid(vars(metric), vars(method)) + theme(aspect.ratio = 1)
```


### Credible Sets

We look at the $95\%$ credible sets for SuSiE and logistic SuSie. Pretty much all of the small credible sets are singleton credible sets, but logistic SuSiE has fewer false positives.

```{r}
pattern <- '^l1.sim.score.cs_([^_])*$'
result <- tar_agg(pattern)

result %>% select(cs_in_causal, n_causal, cs_size, method) %>%
  unnest(c(cs_in_causal, n_causal, cs_size)) %>%
  mutate(contains_causal = n_causal > 0) %>%
  ggplot(aes(x=cs_size, fill=contains_causal)) +
  geom_histogram(position='dodge') + facet_wrap(vars(method)) + theme(aspect=1.0) +
  labs(title='Credible Set Sizes')
```


```{r}
result %>% rowwise() %>% mutate(
    TP = length(unique(unlist(cs_in_causal[cs_size < 20]))),
    FN = 1 - TP,
    FP = sum((n_causal == 0) & (cs_size < 20)),
    recall = TP / (TP + FN),
    precision = TP / (TP + FP),
    F1 = 2 * precision * recall / (precision + recall)
  ) %>% 
  select(background.logit, active.logit, method, recall, precision, F1) %>%
  pivot_longer(c(recall, precision, F1), names_to= 'metric') %>%
  ggplot(aes(x=background.logit, y=active.logit, fill=value)) + 
  geom_tile() + facet_grid(vars(metric), vars(method)) + theme(aspect.ratio = 1) + 
  labs(title='95% Credible sets, Single Active Gene Set')
```


```{r}
result %>% filter(method %in% c('susie.veb.boost', 'logistic.susie.veb.boost')) %>%
  select(method, background.logit, active.logit, rep, batch, cs_size) %>% unnest(cs_size) %>%
  mutate(small_cset = cs_size < 20) %>%
  group_by(method, background.logit, active.logit, rep, batch) %>%
  summarise(n_csets = sum(small_cset)) %>%
  ggplot(aes(x=n_csets, fill=method)) + geom_bar(position = 'dodge2') + theme(aspect.ratio = 1) + 
  labs(title='# of Small Credible sets (< 20)')
```

Virtually all of 

## Multiple active gene sets

In these simulations we have 5 or 10 active gene sets. SuSiE type meothods are fit with $L=10$. Simulations from a model where log odds are additive, intercept determined by `background.logit`, the log odds of observing a gene are `k * active.logit` where $k$ is the number of active gene sets to which that gene is a member.

```{r}
pattern <- '^cas.score_([^_])*$'
result <- tar_agg(pattern)
```

```{r}
plot.data <- result %>%
  unnest(scores) %>%
  group_by(method, n_active, thresh) %>%
  arrange(desc(thresh)) %>%
  mutate(power = sum(TP) / sum(TP + FN), FDP = sum(FP) / sum(FP + TP))

ggplot(data=plot.data, aes(x=FDP, y=power, color=method)) + 
  geom_path() + 
  geom_point(data=subset(plot.data, near(thresh, 0.95))) +
  facet_wrap(vars(n_active)) + theme(aspect=1)
```

```{r}
plot.data <- result %>%
  unnest(scores) %>%
  group_by(method, background.logit, active.logit, thresh) %>%
  arrange(desc(thresh)) %>%
  mutate(power = sum(TP) / sum(TP + FN), FDP = sum(FP) / sum(FP + TP))

ggplot(data=plot.data, aes(x=FDP, y=power, color=method)) + 
  geom_path() + 
  geom_point(data=subset(plot.data, near(thresh, 0.95))) +
  facet_grid(vars(active.logit), vars(background.logit)) + theme(aspect=1.0)
```

We see overall the same trend where SuSiE's curve is better that logistic SuSiE, but it's PIPs are not well calibrated. In all cases logistic SuSiE keeps the proportion of false discoveries low. SuSiE does not.

### 5 active components

```{r}
t <- 0.95
result %>%
  unnest(scores) %>%
  filter(n_active == 5) %>%
  mutate(thresh = ifelse(method %in% c('elastic.net', 'lasso'), thresh + t, thresh)) %>%
  filter(near(thresh, t)) %>%
  group_by(method, background.logit, active.logit) %>%
  summarise(
    recall = sum(TP) / sum(TP + FN),
    precision = sum(TP) / sum(TP + FP),
    specificity = sum(TN) / sum(TN + FP),
    F1 = 2 * precision * recall / (precision + recall)
  ) %>%
  pivot_longer(c(recall, precision, specificity, F1), names_to= 'metric') %>%
  ggplot(aes(x=background.logit, y=active.logit, fill=value)) + 
  geom_tile() + facet_grid(vars(metric), vars(method)) + theme(aspect.ratio = 1)
```


### 10 active components

```{r}
t <- 0.95
result %>%
  unnest(scores) %>%
  filter(n_active == 10) %>%
  mutate(thresh = ifelse(method %in% c('elastic.net', 'lasso'), thresh + t, thresh)) %>%
  filter(near(thresh, t)) %>%
  group_by(method, background.logit, active.logit) %>%
  summarise(
    recall = sum(TP) / sum(TP + FN),
    precision = sum(TP) / sum(TP + FP),
    specificity = sum(TN) / sum(TN + FP),
    F1 = 2 * precision * recall / (precision + recall)
  ) %>%
  pivot_longer(c(recall, precision, specificity, F1), names_to= 'metric') %>%
  ggplot(aes(x=background.logit, y=active.logit, fill=value)) + 
  geom_tile() + facet_grid(vars(metric), vars(method)) + theme(aspect.ratio = 1)
```

### Credible sets

```{r}
pattern <- '^cas.score.cs_([^_])*$'
result <- tar_agg(pattern)

result %>% select(cs_in_causal, n_causal, cs_size, method) %>%
  unnest(c(cs_in_causal, n_causal, cs_size)) %>%
  mutate(contains_causal = n_causal > 0) %>%
  ggplot(aes(x=cs_size, fill=contains_causal)) +
  geom_histogram(position='dodge') + facet_wrap(vars(method)) + theme(aspect=1.0)
```


```{r}
result %>% select(cs_in_causal, n_causal, cs_size, method) %>%
  unnest(c(cs_in_causal, n_causal, cs_size)) %>% 
  mutate(contains_causal = n_causal > 0) %>%
  filter(cs_size < 5) %>%
  ggplot(aes(x=cs_size, fill=contains_causal)) +
  geom_histogram(position='dodge') + facet_wrap(vars(method)) + theme(aspect=1.0)
```

Just a quick look at which credible sets have active gene sets. Virtually all of the small credible sets are singletons, more of the logistic SuSiE gene sets are false positives.

```{r}
result %>% rowwise() %>% mutate(
    TP = length(unique(unlist(cs_in_causal[cs_size < 5]))),
    FN = n_active - TP,
    FP = sum((n_causal == 0) & (cs_size < 5)),
    recall = TP / (TP + FN),
    precision = TP / (TP + FP),
  ) %>% 
  select(n_active, background.logit, active.logit, method, recall, precision) %>%
  pivot_longer(c(recall, precision), names_to= 'metric') %>%
  ggplot(aes(x=background.logit, y=active.logit, fill=value)) + 
  geom_tile() + facet_grid(vars(metric), vars(method)) + theme(aspect.ratio = 1)


result %>% rowwise() %>% mutate(
    TP = length(unique(unlist(cs_in_causal[cs_size < 5]))),
    FN = n_active - TP,
    FP = sum((n_causal == 0) & (cs_size < 5)),
    recall = TP / (TP + FN),
    precision = TP / (TP + FP),
  ) %>% 
  select(n_active, background.logit, active.logit, method, recall, precision) %>%
  pivot_longer(c(recall, precision), names_to= 'metric') %>%
  ggplot(aes(x=metric, y=value, fill=method)) + geom_boxplot() + facet_wrap(vars(active.logit)) + theme(aspect=1.0)
```


```{r}
result %>% filter(method %in% c('susie.veb.boost', 'logistic.susie.veb.boost')) %>%
  select(method, background.logit, active.logit, n_active, rep, batch, cs_size) %>% unnest(cs_size) %>%
  mutate(small_cset = cs_size < 20) %>%
  group_by(method, background.logit, active.logit, n_active, rep, batch) %>%
  summarise(n_csets = sum(small_cset)) %>%
  ggplot(aes(x=n_csets, fill=method)) + geom_bar(position = 'dodge2') + theme(aspect.ratio = 1) + facet_wrap(vars(n_active))
  labs(title='# of Small Credible sets (< 20)')
```

SuSiE is more prone to having extra credible sets.



