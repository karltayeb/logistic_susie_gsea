---
title: "simulations"
author: "karltayeb"
date: "2021-04-13"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

# Simulations

## Overview of Analysis

Here we show the results of 3 simple simulations.
* Single active gene set, gene in gene list iff gene in active gene set
* Single active gene set: gene set genes are included in gene list according to `active.logit`, background genes included in gene set according to `background.logit`
* Multiple active gene sets: 5 or 10 active gene sets per simulation. We simulate under an additive model with intercept determined by `background.logit` and with constant effect size parameterized by `active.logit`

```{r}
knitr::knit_exit()
```

We compare to Fisher's exact test, Regularized regression: elastic net, lasso, and sparse Bayesian regression: logistic SuSiE, SuSiE, Mr. Ash (initilized with a lasso prior)

## Summary

SuSiE and logistic SuSiE worked better than Mr. Ash, but that is possible an issue of setting the priors right.

Logistic SuSiE's PIPs are better calibrated that SuSiE. Although this is not surprising


```{r}
library(targets)
library(tidyverse)

# load and row_bind all targets matching a pattern
tar_agg <- function(pattern){
  load.env <- new.env()
  tar_load(matches(pattern), envir = load.env)
  result <- grep(pattern , names(load.env), value=TRUE)
  result <- do.call("list", mget(result, envir = load.env))
  result <- bind_rows(result)
}
```

# Fully observed gene set

Let's look at how different methods perform identifying a single, fully observed gene set. That is $y_i = 1 \iff g_i \in G$ where $G$ is the gene set of interest. Regression methods should get this perfectly, you can perfectly fit the gene list with $y = {\bf e}_j X$. It's interesting that elastic net fails at this.

The question here is pretty simple, do we detect the true active gene set? And how many other gene sets does each method report?

```{r}
pattern <- '^c2.single.geneset.score_([^_])*$'
result <- tar_agg(pattern)

# plot histogram number of false positives
result %>%
  unnest(scores) %>%
  mutate(thresh = ifelse(method %in% c('elastic.net', 'lasso'), thresh + 0.95, thresh)) %>%
  filter(near(thresh, 0.95)) %>%
  group_by(method) %>%
  mutate(FP = ifelse(FP > 5, 5, FP)) %>%
  group_by(method, FP) %>% summarise(freq = length(FP)) %>%
  ggplot(aes(x=factor(FP), y=freq, fill=method)) + geom_bar(stat="identity") + facet_wrap(vars(method))


# plot
result %>%
  unnest(scores) %>%
  mutate(thresh = ifelse(method %in% c('elastic.net', 'lasso'), thresh + 0.95, thresh)) %>%
  filter(near(thresh, 0.95)) %>%
  group_by(method, TP) %>% summarise(freq = length(TP)) %>%
  ggplot(aes(x=factor(TP), y=freq, fill=method)) + geom_bar(stat="identity") + facet_wrap(vars(method))
```

Elastic net and Fisher's exact test are the only ones that return False positives.
`mr.ash.lasso` is the only one that misses a few of the gene sets. This is odd because it's initialized with lasso.

### Compare SuSiE with VEB boost implimentation

I found that SuSiE was running slower than `veb.boost` when using larger gene set matrices. This is just a check to demonstrate that the PIPs agree.

They do not agree perfectly, but for now I will go ahead using `susie.veb.boost`

```{r}
tar_load(cas.fit_susie, branches = 2)
tar_load(cas.fit_susie.veb.boost, branches = 2)

pip.comp <- tibble(
  susie.pip = cas.fit_susie$pip,
  susie.veb.boost.pip = cas.fit_susie.veb.boost$pip
)

pip.comp %>% unnest(everything()) %>% ggplot(aes(x=susie.pip, y=susie.veb.boost.pip)) + geom_point()
```


## Single active gene set

Now we simulate one active gene set. The log odds of observing the genes in the active gene set are controlled by `active.logit` and the log odds of observing background genes is controlled by `background.logit`

```{r}
pattern <- '^l1.sim.score_([^_])*$'
result <- tar_agg(pattern)

plot.data <- result %>%
  filter(!(method %in% c('elastic.net', 'lasso'))) %>%
  unnest(scores) %>%
  group_by(method, background.logit, active.logit, thresh) %>%
  arrange(dplyr::desc(thresh)) %>%
  mutate(power = sum(TP) / sum(TP + FN), FDP = sum(FP) / sum(FP + TP))

ggplot(data=plot.data, aes(x=FDP, y=power, color=method)) + 
  geom_path() + 
  geom_point(data=subset(plot.data, near(thresh, 0.95))) +
  facet_grid(vars(active.logit), vars(background.logit))
```

Rows are increasing log odds of observing genes in active gene sets. Columns are increasing log odds of observing background genes. Most of these simulation scenarios. Points indicate a PIP threshold of 0.95 (for Fisher's exact test, $< 0.05$ Benjamini Hochberg corrected p-value). Logistic SuSiE and SuSiE do better than Mr. Ash overall. Logistic SuSiE often has lower power at the cutoff, but reliably keeps proportion of false discoveries low.

```{r}
plot.data <- result %>%
  filter(!(method %in% c('elastic.net', 'lasso'))) %>%
  unnest(scores) %>%
  group_by(method, background.logit, thresh) %>%
  arrange(dplyr::desc(thresh)) %>%
  mutate(power = sum(TP) / sum(TP + FN), FDP = sum(FP) / sum(FP + TP))

ggplot(data=plot.data, aes(x=FDP, y=power, color=method)) + 
  geom_path() + 
  geom_point(data=subset(plot.data, near(thresh, 0.95))) +
  facet_wrap(vars(background.logit))
```

This only is a little easier to look at, each facet is a different setting of `background.logit`. Interestingly, SuSiE seems to do a worse job controlling false positives when there are few background genes in the gene list.

```{r}
plot.data <- result %>%
  filter(!(method %in% c('elastic.net', 'lasso'))) %>%
  unnest(scores) %>%
  group_by(method, active.logit, thresh) %>%
  arrange(dplyr::desc(thresh)) %>%
  mutate(power = sum(TP) / sum(TP + FN), FDP = sum(FP) / sum(FP + TP))

ggplot(data=plot.data, aes(x=FDP, y=power, color=method)) + 
  geom_path() + 
  geom_point(data=subset(plot.data, near(thresh, 0.95))) +
  facet_wrap(vars(active.logit))
```


```{r}
t <- 0.95
result %>%
  unnest(scores) %>%
  filter(near(thresh, t)) %>%
  group_by(method, background.logit, active.logit) %>%
  summarise(
    recall = sum(TP) / sum(TP + FN),
    precision = sum(TP) / sum(TP + FP),
    specificity = sum(TN) / sum(TN + FP)
  ) %>%
  pivot_longer(c(recall, precision, specificity), names_to= 'metric') %>%
  ggplot(aes(x=background.logit, y=active.logit, fill=value)) + 
  geom_tile() + facet_grid(vars(metric), vars(method)) + theme(aspect.ratio = 1)
```

Another visualization of each models performance at a 0.95 cutoff. Bottom right corner there is no enrichment.
Mr. Ash is often not reporting any gene sets when `active.logit` is small. We also see that SuSiE has higher recall than logistic SuSiE both when `background.logit` is large and `active.logit` is small.


### Credible Sets


```{r}
pattern <- '^l1.sim.score.cs_([^_])*$'
result <- tar_agg(pattern)

result %>% select(cs_in_causal, n_causal, cs_size, method) %>%
  unnest(c(cs_in_causal, n_causal, cs_size)) %>%
  mutate(contains_causal = n_causal > 0) %>%
  ggplot(aes(x=cs_size, fill=contains_causal)) +
  geom_histogram(position='dodge') + facet_wrap(vars(method)) + theme(aspect=1.0)
```


Pretty much all of the small credible sets are singleton credible sets.


```{r}
result %>% rowwise() %>% mutate(
    TP = length(unique(unlist(cs_in_causal[cs_size < 5]))),
    FN = 1 - TP,
    FP = sum((n_causal == 0) & (cs_size < 5)),
    recall = TP / (TP + FN),
    precision = TP / (TP + FP),
  ) %>% 
  select(background.logit, active.logit, method, recall, precision) %>%
  pivot_longer(c(recall, precision), names_to= 'metric') %>%
  ggplot(aes(x=background.logit, y=active.logit, fill=value)) + 
  geom_tile() + facet_grid(vars(metric), vars(method)) + theme(aspect.ratio = 1)
```


## Multiple active gene sets

In these simulations we have 5 or 10 active gene sets. SuSiE type meothods are fit with $L=10$. Simulations from a model where log odds are additive, intercept determined by `background.logit`, the log odds of observing a gene are `k * active.logit` where $k$ is the number of active gene sets to which that gene is a member.

```{r}
pattern <- '^cas.score_([^_])*$'
result <- tar_agg(pattern)

plot.data <- result %>%
  filter(!(method %in% c('elastic.net', 'lasso'))) %>%
  unnest(scores) %>%
  group_by(method, background.logit, active.logit, thresh) %>%
  arrange(desc(thresh)) %>%
  mutate(power = sum(TP) / sum(TP + FN), FDP = sum(FP) / sum(FP + TP))

ggplot(data=plot.data, aes(x=FDP, y=power, color=method)) + 
  geom_path() + 
  geom_point(data=subset(plot.data, near(thresh, 0.95))) +
  facet_grid(vars(active.logit), vars(background.logit))
```

We see overall the same trend where SuSiE's curve is better that logistic SuSiE, but it's PIPs are not well calibrated. In all cases logistic SuSiE keeps the proportion of false discoveries low. SuSiE does not.

```{r}
plot.data <- result %>%
  filter(!(method %in% c('elastic.net', 'lasso'))) %>%
  unnest(scores) %>%
  group_by(method, n_active, thresh) %>%
  arrange(desc(thresh)) %>%
  mutate(power = sum(TP) / sum(TP + FN), FDP = sum(FP) / sum(FP + TP))

ggplot(data=plot.data, aes(x=FDP, y=power, color=method)) + 
  geom_path() + 
  geom_point(data=subset(plot.data, near(thresh, 0.95))) +
  facet_wrap(vars(n_active)) + theme(aspect=1)
```

Again, this one is probability easier to look at. Facet over number of active gene sets.

### 5 active components

```{r}
t <- 0.95
result %>%
  unnest(scores) %>%
  filter(n_active == 5) %>%
  mutate(thresh = ifelse(method %in% c('elastic.net', 'lasso'), thresh + t, thresh)) %>%
  filter(near(thresh, t)) %>%
  group_by(method, background.logit, active.logit) %>%
  summarise(
    recall = sum(TP) / sum(TP + FN),
    precision = sum(TP) / sum(TP + FP),
    specificity = sum(TN) / sum(TN + FP)
  ) %>%
  pivot_longer(c(recall, precision, specificity), names_to= 'metric') %>%
  ggplot(aes(x=background.logit, y=active.logit, fill=value)) + 
  geom_tile() + facet_grid(vars(metric), vars(method)) + theme(aspect.ratio = 1)
```


### 10 active components

```{r}
t <- 0.95
result %>%
  unnest(scores) %>%
  filter(n_active == 10) %>%
  mutate(thresh = ifelse(method %in% c('elastic.net', 'lasso'), thresh + t, thresh)) %>%
  filter(near(thresh, t)) %>%
  group_by(method, background.logit, active.logit) %>%
  summarise(
    recall = sum(TP) / sum(TP + FN),
    precision = sum(TP) / sum(TP + FP),
    specificity = sum(TN) / sum(TN + FP)
  ) %>%
  pivot_longer(c(recall, precision, specificity), names_to= 'metric') %>%
  ggplot(aes(x=background.logit, y=active.logit, fill=value)) + 
  geom_tile() + facet_grid(vars(metric), vars(method)) + theme(aspect.ratio = 1)
```

### Credible sets

```{r}
pattern <- '^cas.score.cs_([^_])*$'
result <- tar_agg(pattern)

result %>% select(cs_in_causal, n_causal, cs_size, method) %>%
  unnest(c(cs_in_causal, n_causal, cs_size)) %>%
  mutate(contains_causal = n_causal > 0) %>%
  ggplot(aes(x=cs_size, fill=contains_causal)) +
  geom_histogram(position='dodge') + facet_wrap(vars(method)) + theme(aspect=1.0)
```


```{r}
result %>% select(cs_in_causal, n_causal, cs_size, method) %>%
  unnest(c(cs_in_causal, n_causal, cs_size)) %>% 
  mutate(contains_causal = n_causal > 0) %>%
  filter(cs_size < 5) %>%
  ggplot(aes(x=cs_size, fill=contains_causal)) +
  geom_histogram(position='dodge') + facet_wrap(vars(method)) + theme(aspect=1.0)
```

Just a quick look at which credible sets have active gene sets. Virtually all of the small credible sets are singletons, more of the logistic SuSiE gene sets are false positives.

```{r}

result %>% rowwise() %>% mutate(
    TP = length(unique(unlist(cs_in_causal[cs_size < 5]))),
    FN = n_active - TP,
    FP = sum((n_causal == 0) & (cs_size < 5)),
    recall = TP / (TP + FN),
    precision = TP / (TP + FP),
  ) %>% 
  select(n_active, background.logit, active.logit, method, recall, precision) %>%
  pivot_longer(c(recall, precision), names_to= 'metric') %>%
  ggplot(aes(x=background.logit, y=active.logit, fill=value)) + 
  geom_tile() + facet_grid(vars(metric), vars(method)) + theme(aspect.ratio = 1)


result %>% rowwise() %>% mutate(
    TP = length(unique(unlist(cs_in_causal[cs_size < 5]))),
    FN = n_active - TP,
    FP = sum((n_causal == 0) & (cs_size < 5)),
    recall = TP / (TP + FN),
    precision = TP / (TP + FP),
  ) %>% 
  select(n_active, background.logit, active.logit, method, recall, precision) %>%
  pivot_longer(c(recall, precision), names_to= 'metric') %>%
  ggplot(aes(x=metric, y=value, fill=method)) + geom_boxplot() + facet_wrap(vars(active.logit))
```



